{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMcNV0d22rmVB+LjrFDnfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nerve-98/Machine_Learning/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLxkvFl8B4QD"
      },
      "source": [
        "#Import MNIST datasets with 0 and 6 labels\r\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\r\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\r\n",
        "\r\n",
        "idx =  (train_dataset.targets==0) |  (train_dataset.targets==6)\r\n",
        "train_dataset.targets =train_dataset.targets[idx]\r\n",
        "train_dataset.data = train_dataset.data[idx]\r\n",
        "\r\n",
        "test_idx =  (test_dataset.targets==0) |  (test_dataset.targets==6)\r\n",
        "test_dataset.targets =test_dataset.targets[test_idx]\r\n",
        "test_dataset.data = test_dataset.data[test_idx]\r\n",
        "#train_loader\r\n",
        "batch_size=32\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=False)\r\n",
        "#VAE model\r\n",
        "class VAE(nn.Module):\r\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\r\n",
        "        super(VAE, self).__init__()\r\n",
        "        \r\n",
        "        # encoder part\r\n",
        "        self.fc1 = nn.Linear(x_dim, h_dim1)\r\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\r\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)\r\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)\r\n",
        "        # decoder part\r\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim2)\r\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\r\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\r\n",
        "        \r\n",
        "    def encoder(self, x):\r\n",
        "        h = F.relu(self.fc1(x))\r\n",
        "        h = F.relu(self.fc2(h))\r\n",
        "        return self.fc31(h), self.fc32(h) # mu, log_var\r\n",
        "    \r\n",
        "    def sampling(self, mu, log_var):\r\n",
        "        std = torch.exp(0.5*log_var)\r\n",
        "        eps = torch.randn_like(std)\r\n",
        "        return eps.mul(std).add_(mu) # return z sample\r\n",
        "        \r\n",
        "    def decoder(self, z):\r\n",
        "        h = F.relu(self.fc4(z))\r\n",
        "        h = F.relu(self.fc5(h))\r\n",
        "        return F.sigmoid(self.fc6(h)) \r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        mu, log_var = self.encoder(x.view(-1, 784))\r\n",
        "        z = self.sampling(mu, log_var)\r\n",
        "        return self.decoder(z), mu, log_var\r\n",
        "        # build model\r\n",
        "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=3)\r\n",
        "vae.to(device)\r\n",
        "\r\n",
        "# optimizer\r\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\r\n",
        "\r\n",
        "# return Reconstruction Loss\r\n",
        "def loss_function(recon_x, x, mu, log_var):\r\n",
        "    Recons_loss = F.mse_loss(recon_x,x.view(-1,784), size_average=False).div(batch_size)\r\n",
        "    return Recons_loss\r\n",
        "\r\n",
        "    #VAE train\r\n",
        "def train(epoch):\r\n",
        "    vae.train()\r\n",
        "    train_loss = 0\r\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\r\n",
        "        data = data.to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        recon_batch, mu, log_var = vae(data)\r\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        train_loss += loss.item()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        if batch_idx % 100 == 0:\r\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\r\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\r\n",
        "\r\n",
        "#VAE test\r\n",
        "def test():\r\n",
        "    vae.eval()\r\n",
        "    test_loss= 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, _ in test_loader:\r\n",
        "            data = data.to(device)\r\n",
        "            recon, mu, log_var = vae(data)\r\n",
        "            \r\n",
        "            # sum up batch loss\r\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\r\n",
        "        \r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}